# -*- coding: utf-8 -*-
"""Fraud_Detection_AWS (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1neZPBBoPJyCLXfT-y5ZPyht3y54s6Vp7
"""

import boto3
import pandas as pd

# Your S3 bucket and dataset path
s3_path = 's3://fraud-detection-project-cc/creditcard.csv'

# Load dataset directly from S3
df = pd.read_csv(s3_path)

# Preview first few rows
df.head()

df.shape

null_values = df.isnull().sum()

duplicates = df.duplicated()
print("Are there any duplicate rows?", duplicates.any())

# 2️⃣ Count total number of duplicate rows
print("Number of duplicate rows:", duplicates.sum())

# 3️⃣ Display duplicate rows (if any)
duplicate_rows = df[df.duplicated()]
print("Duplicate rows:")
print(duplicate_rows)

#  Drop duplicate rows
df = df.drop_duplicates()

#  Check again after dropping
print("Number of duplicate rows after:", df.duplicated().sum())

#  Reset index after dropping duplicates
#df = df.reset_index(drop=True)

#  Display the cleaned dataset
#print("Cleaned dataset:")
#print(df.head())

print("Shape of data:", df.shape)

print("Columns:", df.columns.tolist())

print(df['Class'].value_counts())

!pip install --upgrade pip
!pip install boto3 sagemaker pandas numpy scikit-learn matplotlib seaborn
!pip install xgboost imbalanced-learn joblib
!pip install plotly tqdm

import boto3, sagemaker, pandas, numpy, sklearn, matplotlib, seaborn, xgboost, imblearn, joblib, plotly, tqdm
print("All required modules installed and imported successfully!")

import sklearn
print("scikit-learn version:", sklearn.__version__)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

!pip install --upgrade pip
!pip install boto3 sagemaker pandas numpy scikit-learn matplotlib seaborn xgboost imbalanced-learn joblib plotly tqdm

# Separate features and target
X = df.drop('Class', axis=1)
y = df['Class']

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize and train
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))

!pip install joblib

import boto3

s3 = boto3.client('s3')
bucket_name = "fraud-detection-project-cc"
region = boto3.session.Session().region_name

# Check if the bucket exists, else create
try:
    s3.head_bucket(Bucket=bucket_name)
    print(f" Bucket '{bucket_name}' already exists.")
except:
    s3.create_bucket(
        Bucket=bucket_name,
        CreateBucketConfiguration={'LocationConstraint': region}
    )
    print(f" Bucket '{bucket_name}' created in region {region}.")

import joblib

# Save model locally
joblib.dump(model, 'fraud_model.pkl')

# Upload to S3
s3.upload_file('fraud_model.pkl', bucket_name, 'fraud_model.pkl')

print(f"✅ Model uploaded to s3://{bucket_name}/fraud_model.pkl")

import sagemaker
from sagemaker.sklearn.model import SKLearnModel
import boto3

session = sagemaker.Session()
role = sagemaker.get_execution_role()
bucket_name = "fraud-detection-project-cc"

print("✅ SageMaker session and role ready.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile inference.py
# import joblib
# import os
# import json
# import numpy as np
# 
# def model_fn(model_dir):
#     model_path = os.path.join(model_dir, "fraud_model.pkl")
#     model = joblib.load(model_path)
#     return model
# 
# def input_fn(request_body, content_type='application/json'):
#     data = json.loads(request_body)
#     return np.array(data['features']).reshape(1, -1)
# 
# def predict_fn(input_data, model):
#     prediction = model.predict(input_data)
#     return prediction.tolist()
# 
# def output_fn(prediction, accept='application/json'):
#     return json.dumps({'prediction': int(prediction[0])})

import boto3, botocore

region = "ap-south-1"
sm = boto3.client("sagemaker", region_name=region)

def safe_delete_endpoint_and_config(name: str):
    # delete endpoint if exists
    try:
        sm.delete_endpoint(EndpointName=name)
        print(f"Requested delete for endpoint: {name}")
    except botocore.exceptions.ClientError as e:
        if "Could not find endpoint" in str(e): pass
        else: raise
    # delete endpoint-config if exists
    try:
        sm.delete_endpoint_config(EndpointConfigName=name)
        print(f"Deleted endpoint-config: {name}")
    except botocore.exceptions.ClientError as e:
        if "Could not find endpoint configuration" in str(e): pass
        else: raise

# remove the conflicting ones
safe_delete_endpoint_and_config("fraud-detection-endpoint")
safe_delete_endpoint_and_config("fraud-detection-endpoint-v2")

import tarfile

with tarfile.open("fraud_model.tar.gz", "w:gz") as tar:
    tar.add("fraud_model.pkl", arcname="fraud_model.pkl")

print("Model packaged into fraud_model.tar.gz")

import boto3

bucket_name = "fraud-detection-project-cc"
s3 = boto3.client("s3")

s3.upload_file("fraud_model.tar.gz", bucket_name, "fraud_model.tar.gz")

print("Uploaded fraud_model.tar.gz to S3")

!ls -l